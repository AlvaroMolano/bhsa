{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal pairs\n",
    "\n",
    "## Task\n",
    "\n",
    "We produce all pairs of Greek/Hebrew vocalized lexemes that satisfy these conditions:\n",
    "\n",
    "1. both members of the pair are equally long\n",
    "2. the edit distance between the members of the pair is exactly 1\n",
    "3. either `(p1, p2)` or `(p2, p1)` is in the pair but not both\n",
    "\n",
    "These are the so-called minimal pairs.\n",
    "They can be used to find phonological rules and to train language learners.\n",
    "\n",
    "See http://www.ibiblio.org/bgreek/forum/viewtopic.php?f=17&t=4308&p=29117#p29117.\n",
    "\n",
    "Thanks to Jonathan Robie for pointing me to this question.\n",
    "\n",
    "## Method\n",
    "\n",
    "We use the Levenshtein edit distance to check condition 2.\n",
    "\n",
    "Install Levenshtein module:\n",
    "\n",
    "```\n",
    "sudo -H pip3 install python-Levenshtein\n",
    "```\n",
    "\n",
    "Documentation of [Levenshtein module](http://www.coli.uni-saarland.de/courses/LT1/2011/slides/Python-Levenshtein.html)\n",
    "\n",
    "## Data\n",
    "\n",
    "We pick text-fabric data for Greek and Hebrew.\n",
    "TF files start with a few metadata lines, preceded by a `@`, then have an empty line, and the\n",
    "subsequent lines contain data items, possibly preceded by a number that we ignore:\n",
    "\n",
    "### Greek\n",
    "\n",
    "```\n",
    "@valueType=str\n",
    "@writtenBy=Text-Fabric\n",
    "@dateWritten=2017-03-21T14:55:58Z\n",
    "\n",
    "βίβλος\n",
    "γένεσις\n",
    "Ἰησοῦς\n",
    "Χριστός\n",
    "υἱός\n",
    "```\n",
    "\n",
    "### Hebrew\n",
    "\n",
    "```\n",
    "@node\n",
    "@author=Eep Talstra Centre for Bible and Computer\n",
    "@dataset=BHSA\n",
    "@datasetName=Biblia Hebraica Stuttgartensia Amstelodamensis\n",
    "@email=shebanq@ancient-data.org\n",
    "@encoders=Constantijn Sikkel (QDF), and Dirk Roorda (TF)\n",
    "@valueType=str\n",
    "@version=2017\n",
    "@website=https://shebanq.ancient-data.org\n",
    "@writtenBy=Text-Fabric\n",
    "@dateWritten=2017-10-10T10:55:32Z\n",
    "\n",
    "1437403\tבְּ\n",
    "רֵאשִׁית\n",
    "ברא\n",
    "אֱלֹהִים\n",
    "אֵת\n",
    "הַ\n",
    "```\n",
    "...\n",
    "\n",
    "Note that we do not need the package Text-Fabric to handle this data.\n",
    "However, by using TF it would have been easy to pick up the *node* numbers of the lexemes, and from there\n",
    "to find all occurrences of the lexemes.\n",
    "That way, we could enrich each minimal pair with a set of concrete examples.\n",
    "\n",
    "### Greek\n",
    "We grab raw text-fabric data from [SBLGNT](https://github.com/Dans-labs/text-fabric-data/tree/master/greek/sblgnt),\n",
    "in particular the data that corresponds to the [UnicodeLemma](https://dans-labs.github.io/text-fabric-data/features/greek/sblgnt/0_overview.html) feature.\n",
    "\n",
    "This file gives the lemma for each of the 137000+ word occurrences of the Greek New Testament.\n",
    "After reading this file, we should weed out duplicates, before computing the minimal pairs.\n",
    "\n",
    "### Hebrew\n",
    "We grab raw text-fabric data from [BHSA](https://github.com/ETCBC/bhsa) (this repo), in particular the data that corresponds to the \n",
    "[voc_lex_utf8](https://etcbc.github.io/bhsa/features/hebrew/2017/voc_lex_utf8) feature.\n",
    "\n",
    "This file contains all 9000+ lexemes of the Hebrew Bible.\n",
    "\n",
    "## Result\n",
    "\n",
    "### Greek\n",
    "The result is a tab-delimited file of 608 minimal pairs:\n",
    "\n",
    "```\n",
    "Βαλάκ\tΒαράκ\n",
    "Βόες\tΒόος\n",
    "Γάζα\tγάζα\n",
    "Διάβολος\tδιάβολος\n",
    "Ζάρα\tΘάρα\n",
    "Ζηλωτής\tζηλωτής\n",
    "Θάλασσα\tθάλασσα\n",
    "Κίς\tΚώς\n",
    "Κίς\tδίς\n",
    "Κίς\tτίς\n",
    "Καλός\tκαλός\n",
    "```\n",
    "\n",
    "Note this pair:\n",
    "\n",
    "```\n",
    "βάλλω\tβάλλω\n",
    "```\n",
    "\n",
    "The first member has `ά` = `03AC\tGREEK SMALL LETTER ALPHA WITH TONOS`.\n",
    "\n",
    "The second member has `ά` = `1F71\tGREEK SMALL LETTER ALPHA WITH OXIA`.\n",
    "\n",
    "I leave it to others to deal with this issue of extended normalization of characters.\n",
    "\n",
    "### Hebrew\n",
    "The result is a tab-delimited file of 22000+ minimal pairs:\n",
    "\n",
    "```\n",
    "בְּ\tכְּ\n",
    "בְּ\tכְּ\n",
    "ברא\tקרא\n",
    "ברא\tברך\n",
    "ברא\tבוא\n",
    "ברא\tירא\n",
    "ברא\tקרא\n",
    "ברא\tברח\n",
    "ברא\tברך\n",
    "ברא\tבטא\n",
    "ברא\tברה\n",
    "ברא\tברה\n",
    "ברא\tברר\n",
    "ברא\tבדא\n",
    "ברא\tבזא\n",
    "ברא\tברד\n",
    "ברא\tפרא\n",
    "ברא\tמרא\n",
    "ברא\tברק\n",
    "ברא\tמרא\n",
    "ברא\tברך\n",
    "ברא\tקרא\n",
    "ברא\tברך\n",
    "אֵת\tאֵד\n",
    "אֵת\tאֵם\n",
    "אֵת\tאֵי\n",
    "אֵת\tעֵת\n",
    "אֵת\tחֵת\n",
    "```\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unicodedata import normalize\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFD = 'NFD'\n",
    "\n",
    "# Hebrew\n",
    "REPO = os.path.expanduser('~/github/etcbc/bhsa')\n",
    "TEMP = '{}/_temp'.format(REPO)\n",
    "\n",
    "os.makedirs(TEMP, exist_ok=True)\n",
    "\n",
    "VERSION = '2017'\n",
    "TFDIR = '{}/tf/{}'.format(REPO, VERSION)\n",
    "lexemeDataH = '{}/voc_lex_utf8.tf'.format(TFDIR)\n",
    "pairFileH = '{}/minimalPairsHebrew.tsv'.format(TEMP)\n",
    "\n",
    "# Greek\n",
    "REPO = os.path.expanduser('~/github/Dans-labs/text-fabric-data')\n",
    "TEMP = '{}/_temp'.format(REPO)\n",
    "\n",
    "os.makedirs(TEMP, exist_ok=True)\n",
    "\n",
    "TFDIR = '{}/{}'.format(REPO, 'greek/sblgnt')\n",
    "lexemeDataG = '{}/UnicodeLemma.tf'.format(TFDIR)\n",
    "pairFileG = '{}/minimalPairsGreek.tsv'.format(TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTfData(dataFile):\n",
    "    items = []\n",
    "    with open(dataFile) as d:\n",
    "        inMeta = True\n",
    "        for line in d:\n",
    "            if inMeta:\n",
    "                if not line.startswith('@'):\n",
    "                    inMeta = False\n",
    "                continue\n",
    "            comps = line.rstrip('\\n').split('\\t', 1)\n",
    "            item = comps[1] if len(comps) == 2 else comps[0]\n",
    "            nitem = normalize(NFD, item)\n",
    "            items.append(item)\n",
    "    # weed out duplicates\n",
    "    items = sorted(set(items))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinimalPairs(items):\n",
    "    pairs = []\n",
    "    nItems = len(items)\n",
    "    for i in range(nItems):\n",
    "        for j in range(i, nItems):\n",
    "            itemI = items[i]\n",
    "            itemJ = items[j]\n",
    "            if len(itemI) != len(itemJ): continue\n",
    "            d = distance(itemI, itemJ)\n",
    "            if d == 1:\n",
    "                pairs.append((itemI, itemJ))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePairs(pairs, pFile):\n",
    "    with open(pFile, 'w') as f:\n",
    "        for (item1, item2) in pairs:\n",
    "            f.write('{}\\t{}\\n'.format(item1, item2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greek pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5518\n",
      "608\n"
     ]
    }
   ],
   "source": [
    "lexemesG = readTfData(lexemeDataG)\n",
    "print(len(lexemesG))\n",
    "# this takes 10 seconds or so\n",
    "minimalPairsG = getMinimalPairs(lexemesG)\n",
    "print(len(minimalPairsG))\n",
    "writePairs(minimalPairsG, pairFileG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hebrew pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8094\n",
      "12825\n"
     ]
    }
   ],
   "source": [
    "lexemesH = readTfData(lexemeDataH)\n",
    "print(len(lexemesH))\n",
    "# this takes 10 seconds or so\n",
    "minimalPairsH = getMinimalPairs(lexemesH)\n",
    "print(len(minimalPairsH))\n",
    "writePairs(minimalPairsH, pairFileH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
