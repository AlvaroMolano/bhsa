{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# TF from MQL\n",
    "\n",
    "This notebook can read an\n",
    "[MQL](https://emdros.org/mql.html)\n",
    "dump of a version of the [BHSA](https://github.com/ETCBC/bhsa) Hebrew Text Database\n",
    "and transform it in a Text-Fabric\n",
    "[Text-Fabric](https://github.com/ETCBC/text-fabric)\n",
    "resource.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The principled way of going about such a conversion is to import the MQL source into\n",
    "an [Emdros](https://emdros.org) database, and use it to retrieve objects and features from there.\n",
    "\n",
    "Because the syntax of an MQL file leaves some freedom, it is error prone to do a text-to-text conversion from\n",
    "MQL to something else.\n",
    "\n",
    "Yet this is what we do, the error-prone thing. We then avoid installing and configuring and managing Emdros, MySQL/sqLite3.\n",
    "Aside the upfront work to get this going, the going after that is also much slower.\n",
    "\n",
    "So here you are, a smallish script to do an awful lot of work, mostly correct, if careful used.\n",
    "\n",
    "# Caveat\n",
    "\n",
    "This notebook makes use of a new feature of text-fabric, first present in 2.3.12.\n",
    "Make sure to upgrade first.\n",
    "\n",
    "```sudo -H pip3 install text-fabric\n",
    "```\n",
    "\n",
    "# Buffer function\n",
    "The ETCBC does not yet produce an MQL file that satisfies all the requirements.\n",
    "Some features are still missing, some values seem to have been mangled somewhere in the creation workflow.\n",
    "\n",
    "This pipeline implements workarounds for those issues.\n",
    "The source data, as delivered by the ETCBC on a weekly basis, may change suddenly in minor details,\n",
    "which could break applications further down the line.\n",
    "\n",
    "This pipeline, with and in particular this repository is a useful tool to work around those issues\n",
    "temporarily and to provide feedback to the ETCBC, which will hopefully lead to a more \n",
    "consistent data interface over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "from shutil import rmtree\n",
    "from tf.fabric import Fabric\n",
    "from tf.helpers import setFromSpec\n",
    "import utils\n",
    "from blang import bookLangs, bookNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "See [operation](https://github.com/ETCBC/pipeline/blob/master/README.md#operation) \n",
    "for how to run this script in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'SCRIPT' not in locals():\n",
    "    SCRIPT = False\n",
    "    FORCE = True\n",
    "    CORE_NAME = 'bhsa'\n",
    "    VERSION = 'c'\n",
    "    CORE_MODULE ='core' \n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT: sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "module = CORE_MODULE\n",
    "repoBase = os.path.expanduser('~/github/etcbc')\n",
    "thisRepo = '{}/{}'.format(repoBase, CORE_NAME)\n",
    "\n",
    "thisSource = '{}/source/{}'.format(thisRepo, VERSION)\n",
    "mqlzFile = '{}/{}.mql.bz2'.format(thisSource, CORE_NAME)\n",
    "\n",
    "thisTemp = '{}/_temp/{}'.format(thisRepo, VERSION)\n",
    "mqlFile = '{}/{}.mql'.format(thisTemp, CORE_NAME)\n",
    "thisSave = '{}/{}'.format(thisTemp, module)\n",
    "\n",
    "thisTf = '{}/tf/{}'.format(thisRepo, VERSION)\n",
    "thisDeliver = '{}/{}'.format(thisTf, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    testFile = '{}/.tf/otype.tfx'.format(thisDeliver)\n",
    "    (good, work) = utils.mustRun(mqlzFile, '{}/.tf/otype.tfx'.format(thisDeliver), force=FORCE)\n",
    "    if not good: stop(good=False)\n",
    "    if not work: stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Settings\n",
    "\n",
    "We add some custom information here.\n",
    "\n",
    "* the MQL object type that corresponds to the TF slot type, typically `word`;\n",
    "* a piece of metadata that will go into every feature; the time will be added automatically\n",
    "* suitable text formats for the `otext` feature of TF.\n",
    "\n",
    "The oText feature is very sensitive to what is available in the source MQL.\n",
    "It needs to be configured here.\n",
    "We save the configs we need per source and version.\n",
    "And we define a stripped down default version to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slotType = 'word'\n",
    "\n",
    "featureMetadata = dict(\n",
    "    dataset='BHSA',\n",
    "    datasetName='Biblia Hebraica Stuttgartensia Amstelodamensis',\n",
    "    author='Eep Talstra Centre for Bible and Computer',\n",
    "    encoders='Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)',\n",
    "    website='https://shebanq.ancient-data.org',\n",
    "    email='shebanq@ancient-data.org',\n",
    ")\n",
    "\n",
    "oText = {\n",
    "    '': {\n",
    "        '': '''\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "@fmt:text-orig-full={g_word_utf8}{g_suffix_utf8}\n",
    "''',\n",
    "    },\n",
    "    '4': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_qere_utf8/g_word_utf8}{qtrailer_utf8/trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word} \n",
    "@fmt:text-trans-full-ketiv={g_word} \n",
    "@fmt:text-trans-plain={g_cons} \n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "''',\n",
    "    '4b': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_qere_utf8/g_word_utf8}{qtrailer_utf8/trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word} \n",
    "@fmt:text-trans-full-ketiv={g_word} \n",
    "@fmt:text-trans-plain={g_cons} \n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "''',\n",
    "    'c': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word}{trailer}\n",
    "@fmt:text-trans-plain={g_cons}{trailer}\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "''',\n",
    "    '2017': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word}{trailer}\n",
    "@fmt:text-trans-plain={g_cons}{trailer}\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "''',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function selects the proper otext material, falling back on a default if nothing \n",
    "appropriate has been specified in `oText`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getOtext():\n",
    "    thisOtext = oText.get(VERSION, oText[''])\n",
    "    otextInfo = dict(line[1:].split('=', 1) for line in thisOtext.strip('\\n').split('\\n'))\n",
    "\n",
    "    if thisOtext is oText['']:\n",
    "        utils.caption(0, 'WARNING: no otext feature info provided, using a meager default value') \n",
    "    else:\n",
    "        utils.caption(0, 'INFO: otext feature information found')\n",
    "    for x in sorted(otextInfo.items()):\n",
    "        utils.caption(0, '\\t{:<20} = \"{}\"'.format(*x))\n",
    "    return otextInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The program has several stages:\n",
    "   \n",
    "1. **prepare** the source (utils.bunzip if needed)\n",
    "1. **parse MQL** and collect information in datastructures\n",
    "1. **transform to TF** write the datastructures as TF features\n",
    "1. **differences** (informational)\n",
    "1. **deliver** the tf data at its destination directory\n",
    "1. **compile** all tf features to binary format\n",
    "\n",
    "Stages **parseMQL** and **transform to TF** communicate with the help of several global variables:\n",
    "\n",
    "* data containers for the MQL kinds of data\n",
    "  * enumerations\n",
    "  * object types\n",
    "  * tables\n",
    "\n",
    "* data containers for the TF features to be generated,\n",
    "  * node features\n",
    "  * edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objectTypes = dict()\n",
    "tables = dict()\n",
    "\n",
    "edgeF = dict()\n",
    "nodeF = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Prepare\n",
    "\n",
    "Check the source, utils.bunzip it if needed, empty the result directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    global thisoText\n",
    "\n",
    "    if not os.path.exists(thisTemp):\n",
    "        os.makedirs(thisTemp)\n",
    "\n",
    "    utils.caption(0, 'bunzipping {} ...'.format(mqlzFile))\n",
    "    utils.bunzip(mqlzFile, mqlFile)\n",
    "    utils.caption(0, 'Done')\n",
    "\n",
    "    if os.path.exists(thisSave): rmtree(thisSave)\n",
    "    os.makedirs(thisSave)\n",
    "\n",
    "    thisoText = getOtext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a monads specification (a comma separated sequence of numbers and number ranges)\n",
    "into a set of integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: MQL parsing\n",
    "Plough through the MQL file and grab all relevant information\n",
    "and put it into the dedicated data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uniscan = re.compile(r'(?:\\\\x..)+')\n",
    "\n",
    "def makeuni(match):\n",
    "    ''' Make proper unicode of a text that contains byte escape codes such as backslash xb6\n",
    "    '''\n",
    "    byts = eval('\"' + match.group(0) + '\"')\n",
    "    return byts.encode('latin1').decode('utf-8')\n",
    "\n",
    "def uni(line): return uniscan.sub(makeuni, line)\n",
    "    \n",
    "def parseMql():\n",
    "    utils.caption(4, 'Parsing mql source ...')\n",
    "    fh = open(mqlFile)\n",
    "\n",
    "    curId = None\n",
    "    curEnum = None\n",
    "    curObjectType = None\n",
    "    curTable = None\n",
    "    curObject = None\n",
    "    curValue = None\n",
    "    curFeature = None\n",
    "\n",
    "    STRING_TYPES = {'ascii', 'string'}\n",
    "\n",
    "    enums = dict()\n",
    "\n",
    "    chunkSize = 1000000\n",
    "    inThisChunk = 0\n",
    "\n",
    "    good = True\n",
    "\n",
    "    for (ln, line) in enumerate(fh):\n",
    "        inThisChunk += 1\n",
    "        if inThisChunk == chunkSize:\n",
    "            utils.caption(0, '\\tline {:>9}'.format(ln + 1))\n",
    "            inThisChunk = 0\n",
    "        if line.startswith('CREATE OBJECTS WITH OBJECT TYPE') or line.startswith('WITH OBJECT TYPE'):\n",
    "            comps = line.rstrip().rstrip(']').split('[', 1)\n",
    "            curTable = comps[1]\n",
    "            utils.caption(0, '\\t\\tobjects in {}'.format(curTable))\n",
    "            curObject = None\n",
    "            if not curTable in tables:\n",
    "                tables[curTable] = dict()\n",
    "        elif curEnum != None:\n",
    "            if line.startswith('}'):\n",
    "                curEnum = None\n",
    "                continue\n",
    "            comps = line.strip().rstrip(',').split('=', 1)\n",
    "            comp = comps[0].strip()\n",
    "            words = comp.split()\n",
    "            if words[0] == 'DEFAULT':\n",
    "                enums[curEnum]['default'] = uni(words[1])\n",
    "                value = words[1]\n",
    "            else:\n",
    "                value = words[0]\n",
    "            enums[curEnum]['values'].append(value)\n",
    "        elif curObjectType != None:\n",
    "            if line.startswith(']'):\n",
    "                curObjectType = None\n",
    "                continue\n",
    "            if curObjectType == True:\n",
    "                if line.startswith('['):\n",
    "                    curObjectType = line.rstrip()[1:]\n",
    "                    objectTypes[curObjectType] = dict()\n",
    "                    utils.caption(0, '\\t\\totype {}'.format(curObjectType))\n",
    "                    continue\n",
    "            comps = line.strip().rstrip(';').split(':', 1)\n",
    "            feature = comps[0].strip()\n",
    "            fInfo = comps[1].strip()\n",
    "            fCleanInfo = fInfo.replace('FROM SET', '')\n",
    "            fInfoComps = fCleanInfo.split(' ', 1)\n",
    "            fMQLType = fInfoComps[0]\n",
    "            fDefault = fInfoComps[1].strip().split(' ', 1)[1] if len(fInfoComps) == 2 else None\n",
    "            if fDefault != None and fMQLType in STRING_TYPES:\n",
    "                fDefault = uni(fDefault[1:-1])\n",
    "            default = enums.get(fMQLType, {}).get('default', fDefault)\n",
    "            ftype = 'str' if fMQLType in enums else\\\n",
    "                    'int' if fMQLType == 'integer' else\\\n",
    "                    'str' if fMQLType in STRING_TYPES else\\\n",
    "                    'int' if fInfo == 'id_d' else\\\n",
    "                    'str'\n",
    "            isEdge = fMQLType == 'id_d'\n",
    "            if isEdge:\n",
    "                edgeF.setdefault(curObjectType, set()).add(feature)\n",
    "            else:\n",
    "                nodeF.setdefault(curObjectType, set()).add(feature)\n",
    "\n",
    "            objectTypes[curObjectType][feature] = (ftype, default)\n",
    "            utils.caption(0, '\\t\\t\\tfeature {} ({}) =def= {} : {}'.format(feature, ftype, default, 'edge' if isEdge else 'node'))\n",
    "        elif curTable != None:\n",
    "            if curObject != None:\n",
    "                if line.startswith(']'):\n",
    "                    objectType = objectTypes[curTable]\n",
    "                    for (feature, (ftype, default)) in objectType.items():\n",
    "                        if feature not in curObject['feats'] and default != None:\n",
    "                            curObject['feats'][feature] = default\n",
    "                    tables[curTable][curId] = curObject\n",
    "                    curObject = None\n",
    "                    continue\n",
    "                elif line.startswith('['):\n",
    "                    continue\n",
    "                elif line.startswith('FROM MONADS'):\n",
    "                    monads = line.split('=', 1)[1].replace('{', '').replace('}', '').replace(' ','').strip()\n",
    "                    curObject['monads'] = setFromSpec(monads)\n",
    "                elif line.startswith('WITH ID_D'):\n",
    "                    comps = line.replace('[', '').rstrip().split('=', 1)\n",
    "                    curId = int(comps[1])\n",
    "                elif line.startswith('GO'):\n",
    "                    continue\n",
    "                elif line.strip() == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    if curValue != None:\n",
    "                        toBeContinued = not line.rstrip().endswith('\";')\n",
    "                        if toBeContinued:\n",
    "                            curValue += line\n",
    "                        else:\n",
    "                            curValue += line.rstrip().rstrip(';').rstrip('\"')\n",
    "                            curObject['feats'][curFeature] = uni(curValue)\n",
    "                            curValue = None\n",
    "                            curFeature = None\n",
    "                        continue\n",
    "                    if ':=' in line:\n",
    "                        (featurePart, valuePart) = line.split('=', 1)\n",
    "                        feature = featurePart[0:-1].strip()\n",
    "                        isText = ':=\"' in line\n",
    "                        toBeContinued = isText and not line.rstrip().endswith('\";')\n",
    "                        if toBeContinued:\n",
    "                            # this happens if a feature value contains a new line\n",
    "                            # we must continue scanning lines until we meet the ned of the value\n",
    "                            curFeature = feature\n",
    "                            curValue = valuePart.lstrip('\"')\n",
    "                        else:\n",
    "                            value = valuePart.rstrip().rstrip(';').strip('\"')\n",
    "                            curObject['feats'][feature] = uni(value) if isText else value\n",
    "                    else:\n",
    "                        utils.caption(0, 'ERROR: line {}: unrecognized line -->{}<--'.format(ln, line))\n",
    "                        good = False\n",
    "                        break\n",
    "            else:\n",
    "                if line.startswith('CREATE OBJECT'):\n",
    "                    curObject = dict(feats=dict(), monads=None)\n",
    "                    curId = None\n",
    "        else:\n",
    "            if line.startswith('CREATE ENUMERATION'):\n",
    "                words = line.split()\n",
    "                curEnum = words[2]\n",
    "                enums[curEnum] = dict(default=None, values=[])\n",
    "                utils.caption(0, '\\t\\tenum {}'.format(curEnum))\n",
    "            elif line.startswith('CREATE OBJECT TYPE'):\n",
    "                curObjectType = True\n",
    "    utils.caption(0, '{} lines parsed'.format(ln + 1))\n",
    "    fh.close()\n",
    "    for table in tables:\n",
    "        utils.caption(0, '{} objects of type {}'.format(len(tables[table]), table))\n",
    "    if not good:\n",
    "        stop(good=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: TF generation\n",
    "Transform the collected information in feature-like datastructures, and write it all\n",
    "out to `.tf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tfFromData():\n",
    "    utils.caption(4, 'Making TF data ...')\n",
    "    \n",
    "    NIL = {'nil', 'NIL', 'Nil'}\n",
    "\n",
    "    tableOrder = [slotType]+[t for t in sorted(tables) if t != slotType]\n",
    "\n",
    "    nodeFromIdd = dict()\n",
    "    iddFromNode = dict()\n",
    "\n",
    "    nodeFeatures = dict()\n",
    "    edgeFeatures = dict()\n",
    "    metaData = dict()\n",
    "\n",
    "    # metadata that ends up in every feature\n",
    "    metaData[''] = featureMetadata\n",
    "\n",
    "    # the config feature otext\n",
    "    metaData['otext'] = thisoText\n",
    "\n",
    "    # multilingual book names\n",
    "    for (langCode, (langEnglish, langName)) in bookLangs.items():\n",
    "        metaData['book@{}'.format(langCode)] = {\n",
    "            'valueType': 'str',\n",
    "            'language': langName,\n",
    "            'languageCode': langCode,\n",
    "            'languageEnglish': langEnglish,\n",
    "        }\n",
    "\n",
    "    utils.caption(0, 'Monad - idd mapping ...')\n",
    "    otype = dict()\n",
    "    for idd in tables.get(slotType, {}):\n",
    "        monad = list(tables[slotType][idd]['monads'])[0]\n",
    "        nodeFromIdd[idd] = monad\n",
    "        iddFromNode[monad] = idd\n",
    "        otype[monad] = slotType\n",
    "\n",
    "    maxSlot = max(nodeFromIdd.values()) if len(nodeFromIdd) else 0\n",
    "    utils.caption(0, 'maxSlot={}'.format(maxSlot))\n",
    "\n",
    "    utils.caption(0, 'Node mapping and otype ...')\n",
    "    node = maxSlot\n",
    "    for t in tableOrder[1:]:\n",
    "        for idd in sorted(tables[t]):\n",
    "            node += 1\n",
    "            nodeFromIdd[idd] = node\n",
    "            iddFromNode[node] = idd\n",
    "            otype[node] = t\n",
    "\n",
    "    nodeFeatures['otype'] = otype\n",
    "    metaData['otype'] = dict(\n",
    "        valueType='str',\n",
    "    )\n",
    "\n",
    "    utils.caption(0, 'oslots ...')\n",
    "    oslots = dict()\n",
    "    for t in tableOrder[1:]:\n",
    "        for idd in tables.get(t, {}):\n",
    "            node = nodeFromIdd[idd]\n",
    "            monads = tables[t][idd]['monads']\n",
    "            oslots[node] = monads\n",
    "    edgeFeatures['oslots'] = oslots\n",
    "    metaData['oslots'] = dict(\n",
    "        valueType='str',\n",
    "    )\n",
    "\n",
    "    utils.caption(0, 'metadata ...')\n",
    "    for t in nodeF:\n",
    "        for f in nodeF[t]:\n",
    "            ftype = objectTypes[t][f][0]\n",
    "            metaData.setdefault(f, {})['valueType'] = ftype\n",
    "    for t in edgeF:\n",
    "        for f in edgeF[t]:\n",
    "            metaData.setdefault(f, {})['valueType'] = 'str'\n",
    "\n",
    "    utils.caption(4, 'features ...')\n",
    "    chunkSize = 100000\n",
    "    for t in tableOrder:\n",
    "        utils.caption(0, '\\tfeatures from {}s'.format(t))\n",
    "        inThisChunk = 0\n",
    "        for (i, idd) in enumerate(tables.get(t, {})):\n",
    "            inThisChunk += 1\n",
    "            if inThisChunk == chunkSize:\n",
    "                utils.caption(0, '\\t{:>9} {}s'.format(i + 1, t))\n",
    "                inThisChunk = 0\n",
    "            node = nodeFromIdd[idd]\n",
    "            features = tables[t][idd]['feats']\n",
    "            for (f, v) in features.items():\n",
    "                isEdge = f in edgeF.get(t, set())\n",
    "                if isEdge:\n",
    "                    if v not in NIL:\n",
    "                        edgeFeatures.setdefault(f, {}).setdefault(node, set()).add(nodeFromIdd[int(v)])\n",
    "                else:\n",
    "                    nodeFeatures.setdefault(f, {})[node] = v\n",
    "        utils.caption(0, '\\t{:>9} {}s'.format(i + 1, t))\n",
    "\n",
    "    utils.caption(0, 'book names ...')\n",
    "    nodeFeatures['book@la'] = nodeFeatures.get('book', {})\n",
    "    bookNodes = sorted(nodeFeatures.get('book', {}))\n",
    "    for (langCode, langBookNames) in bookNames.items():\n",
    "        nodeFeatures['book@{}'.format(langCode)] = dict(zip(bookNodes, langBookNames))\n",
    "\n",
    "    utils.caption(4, 'write data set to TF ...')\n",
    "\n",
    "    TF = Fabric(locations=thisSave, silent=True)\n",
    "    TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Diffs\n",
    "\n",
    "Check differences with previous versions.\n",
    "\n",
    "The new dataset has been created in a temporary directory,\n",
    "and has not yet been copied to its destination.\n",
    "\n",
    "Here is your opportunity to compare the newly created features with the older features.\n",
    "You expect some differences in some features.\n",
    "\n",
    "We check the differences between the previous version of the features and what has been generated.\n",
    "We list features that will be added and deleted and changed.\n",
    "For each changed feature we show the first line where the new feature differs from the old one.\n",
    "We ignore changes in the metadata, because the timestamp in the metadata will always change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Deliver \n",
    "\n",
    "Copy the new TF dataset from the temporary location where it has been created to its final destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Compile TF\n",
    "\n",
    "Just to see whether everything loads and the precomputing of extra information works out.\n",
    "Moreover, if you want to work with these features, then the precomputing has already been done, and everything is quicker in subsequent runs.\n",
    "\n",
    "We issue load statement to trigger the precomputing of extra data.\n",
    "Note that all features specified text formats in the `otext` config feature,\n",
    "will be loaded, as well as the features for sections.\n",
    "\n",
    "At that point we have access to the full list of features.\n",
    "We grab them and are going to load them all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compileTfData():\n",
    "    utils.caption(4, 'Load and compile standard TF features')\n",
    "    TF = Fabric(locations=thisTf, modules=module)\n",
    "    api = TF.load('')\n",
    "\n",
    "    utils.caption(4, 'Load and compile all other TF features')\n",
    "    allFeatures = TF.explore(silent=False, show=True)\n",
    "    loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "    api = TF.load(loadableFeatures)\n",
    "    T = api.T\n",
    "    \n",
    "    utils.caption(4, 'Basic test')\n",
    "    utils.caption(4, 'First verse in all formats')\n",
    "    for fmt in T.formats:\n",
    "        utils.caption(0, '{}'.format(fmt), continuation=True)\n",
    "        utils.caption(0, '\\t{}'.format(T.text(range(1,12), fmt=fmt)), continuation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s bunzipping /Users/dirk/github/etcbc/bhsa/source/c/bhsa.mql.bz2 ...\n",
      "NOTE: Using existing /Users/dirk/github/etcbc/bhsa/_temp/c/bhsa.mql which is newer than /Users/dirk/github/etcbc/bhsa/source/c/bhsa.mql.bz2\n",
      "      0.00s Done\n",
      "INFO: otext feature information found\n",
      "fmt:lex-orig-full    = \"{g_lex_utf8} \"\n",
      "fmt:lex-orig-plain   = \"{lex_utf8} \"\n",
      "fmt:lex-trans-full   = \"{g_lex} \"\n",
      "fmt:lex-trans-plain  = \"{lex} \"\n",
      "fmt:text-orig-full   = \"{g_word_utf8}{trailer_utf8}\"\n",
      "fmt:text-orig-plain  = \"{g_cons_utf8}{trailer_utf8}\"\n",
      "fmt:text-trans-full  = \"{g_word}{trailer}\"\n",
      "fmt:text-trans-plain = \"{g_cons}{trailer}\"\n",
      "sectionFeatures      = \"book,chapter,verse\"\n",
      "sectionTypes         = \"book,chapter,verse\"\n"
     ]
    }
   ],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.03s Parsing mql source ...\n",
      "\t\tenum boolean_t\n",
      "\t\tenum phrase_determination_t\n",
      "\t\tenum language_t\n",
      "\t\tenum book_name_t\n",
      "\t\tenum lexical_set_t\n",
      "\t\tenum verbal_stem_t\n",
      "\t\tenum verbal_tense_t\n",
      "\t\tenum person_t\n",
      "\t\tenum number_t\n",
      "\t\tenum gender_t\n",
      "\t\tenum state_t\n",
      "\t\tenum part_of_speech_t\n",
      "\t\tenum phrase_type_t\n",
      "\t\tenum phrase_atom_relation_t\n",
      "\t\tenum phrase_relation_t\n",
      "\t\tenum phrase_atom_unit_distance_to_mother_t\n",
      "\t\tenum subphrase_relation_t\n",
      "\t\tenum subphrase_mother_object_type_t\n",
      "\t\tenum phrase_function_t\n",
      "\t\tenum clause_atom_type_t\n",
      "\t\tenum clause_type_t\n",
      "\t\tenum clause_kind_t\n",
      "\t\tenum clause_constituent_relation_t\n",
      "\t\tenum clause_constituent_mother_object_type_t\n",
      "\t\tenum clause_constituent_unit_distance_to_mother_t\n",
      "\t\totype word\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature g_voc_lex (str) = None : node\n",
      "\t\t\tfeature g_vbe_utf8 (str) = None : node\n",
      "\t\t\tfeature g_voc_lex_utf8 (str) = None : node\n",
      "\t\t\tfeature g_nme (str) = None : node\n",
      "\t\t\tfeature nme (str) = None : node\n",
      "\t\t\tfeature g_vbe (str) = None : node\n",
      "\t\t\tfeature g_word (str) = None : node\n",
      "\t\t\tfeature g_cons_utf8 (str) = None : node\n",
      "\t\t\tfeature g_cons (str) = None : node\n",
      "\t\t\tfeature g_word_utf8 (str) = None : node\n",
      "\t\t\tfeature trailer (str) = None : node\n",
      "\t\t\tfeature trailer_utf8 (str) = None : node\n",
      "\t\t\tfeature g_pfm (str) = None : node\n",
      "\t\t\tfeature g_pfm_utf8 (str) = None : node\n",
      "\t\t\tfeature g_vbs (str) = None : node\n",
      "\t\t\tfeature g_vbs_utf8 (str) = None : node\n",
      "\t\t\tfeature lex (str) = None : node\n",
      "\t\t\tfeature lex_utf8 (str) = None : node\n",
      "\t\t\tfeature g_lex (str) = None : node\n",
      "\t\t\tfeature g_lex_utf8 (str) = None : node\n",
      "\t\t\tfeature g_prs_utf8 (str) = None : node\n",
      "\t\t\tfeature g_prs (str) = None : node\n",
      "\t\t\tfeature qere_utf8 (str) = None : node\n",
      "\t\t\tfeature g_uvf_utf8 (str) = None : node\n",
      "\t\t\tfeature g_uvf (str) = None : node\n",
      "\t\t\tfeature qere (str) = None : node\n",
      "\t\t\tfeature g_nme_utf8 (str) = None : node\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\t\tfeature distributional_parent (str) = None : edge\n",
      "\t\t\tfeature pfm (str) = None : node\n",
      "\t\t\tfeature prs (str) = None : node\n",
      "\t\t\tfeature uvf (str) = None : node\n",
      "\t\t\tfeature vbe (str) = None : node\n",
      "\t\t\tfeature vbs (str) = None : node\n",
      "\t\t\tfeature language (str) = Hebrew : node\n",
      "\t\t\tfeature ls (str) = none : node\n",
      "\t\t\tfeature vs (str) = NA : node\n",
      "\t\t\tfeature vt (str) = NA : node\n",
      "\t\t\tfeature prs_ps (str) = NA : node\n",
      "\t\t\tfeature ps (str) = NA : node\n",
      "\t\t\tfeature nu (str) = NA : node\n",
      "\t\t\tfeature prs_nu (str) = NA : node\n",
      "\t\t\tfeature prs_gn (str) = NA : node\n",
      "\t\t\tfeature gn (str) = NA : node\n",
      "\t\t\tfeature st (str) = NA : node\n",
      "\t\t\tfeature sp (str) = art : node\n",
      "\t\t\tfeature pdp (str) = art : node\n",
      "\t\totype clause_atom\n",
      "\t\t\tfeature tab (int) = None : node\n",
      "\t\t\tfeature code (int) = None : node\n",
      "\t\t\tfeature dist (int) = None : node\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature distributional_parent (str) = None : edge\n",
      "\t\t\tfeature mother (str) = None : edge\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\t\tfeature is_root (str) = false : node\n",
      "\t\t\tfeature typ (str) = Unkn : node\n",
      "\t\totype sentence\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\totype sentence_atom\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\totype subphrase\n",
      "\t\t\tfeature mother (str) = None : edge\n",
      "\t\t\tfeature rela (str) = NA : node\n",
      "\t\t\tfeature mother_object_type (str) = NA : node\n",
      "\t\totype phrase\n",
      "\t\t\tfeature dist (int) = None : node\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\t\tfeature mother (str) = None : edge\n",
      "\t\t\tfeature det (str) = NA : node\n",
      "\t\t\tfeature typ (str) = VP : node\n",
      "\t\t\tfeature rela (str) = NA : node\n",
      "\t\t\tfeature dist_unit (str) = clause_atoms : node\n",
      "\t\t\tfeature function (str) = Unkn : node\n",
      "\t\totype chapter\n",
      "\t\t\tfeature chapter (int) = None : node\n",
      "\t\t\tfeature book (str) = Genesis : node\n",
      "\t\totype book\n",
      "\t\t\tfeature book (str) = Genesis : node\n",
      "\t\totype clause\n",
      "\t\t\tfeature dist (int) = None : node\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature domain (str) = None : node\n",
      "\t\t\tfeature mother (str) = None : edge\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\t\tfeature txt (str) = None : node\n",
      "\t\t\tfeature typ (str) = Unkn : node\n",
      "\t\t\tfeature kind (str) = unknown : node\n",
      "\t\t\tfeature rela (str) = NA : node\n",
      "\t\t\tfeature mother_object_type (str) = clause : node\n",
      "\t\t\tfeature dist_unit (str) = clause_atoms : node\n",
      "\t\totype half_verse\n",
      "\t\t\tfeature label (str) = None : node\n",
      "\t\totype verse\n",
      "\t\t\tfeature verse (int) = None : node\n",
      "\t\t\tfeature chapter (int) = None : node\n",
      "\t\t\tfeature label (str) = None : node\n",
      "\t\t\tfeature book (str) = Genesis : node\n",
      "\t\totype phrase_atom\n",
      "\t\t\tfeature number (int) = None : node\n",
      "\t\t\tfeature dist (int) = None : node\n",
      "\t\t\tfeature distributional_parent (str) = None : edge\n",
      "\t\t\tfeature mother (str) = None : edge\n",
      "\t\t\tfeature functional_parent (str) = None : edge\n",
      "\t\t\tfeature det (str) = NA : node\n",
      "\t\t\tfeature typ (str) = VP : node\n",
      "\t\t\tfeature rela (str) = NA : node\n",
      "\t\t\tfeature dist_unit (str) = clause_atoms : node\n",
      "\t\tobjects in word\n",
      "      5.97s \tline   1000000\n",
      "        12s \tline   2000000\n",
      "\t\tobjects in word\n",
      "        18s \tline   3000000\n",
      "        25s \tline   4000000\n",
      "        31s \tline   5000000\n",
      "\t\tobjects in word\n",
      "        36s \tline   6000000\n",
      "        43s \tline   7000000\n",
      "\t\tobjects in word\n",
      "        49s \tline   8000000\n",
      "        57s \tline   9000000\n",
      "     1m 02s \tline  10000000\n",
      "\t\tobjects in word\n",
      "     1m 08s \tline  11000000\n",
      "     1m 14s \tline  12000000\n",
      "     1m 19s \tline  13000000\n",
      "\t\tobjects in word\n",
      "     1m 26s \tline  14000000\n",
      "     1m 33s \tline  15000000\n",
      "\t\tobjects in word\n",
      "     1m 39s \tline  16000000\n",
      "     1m 45s \tline  17000000\n",
      "     1m 50s \tline  18000000\n",
      "\t\tobjects in word\n",
      "     1m 57s \tline  19000000\n",
      "     2m 04s \tline  20000000\n",
      "\t\tobjects in word\n",
      "     2m 10s \tline  21000000\n",
      "     2m 16s \tline  22000000\n",
      "\t\tobjects in clause_atom\n",
      "\t\tobjects in clause_atom\n",
      "     2m 20s \tline  23000000\n",
      "\t\tobjects in sentence\n",
      "\t\tobjects in sentence\n",
      "\t\tobjects in sentence_atom\n",
      "\t\tobjects in sentence_atom\n",
      "     2m 23s \tline  24000000\n",
      "\t\tobjects in subphrase\n",
      "\t\tobjects in subphrase\n",
      "\t\tobjects in subphrase\n",
      "\t\tobjects in phrase\n",
      "     2m 27s \tline  25000000\n",
      "\t\tobjects in phrase\n",
      "     2m 31s \tline  26000000\n",
      "\t\tobjects in phrase\n",
      "\t\tobjects in phrase\n",
      "     2m 34s \tline  27000000\n",
      "\t\tobjects in phrase\n",
      "     2m 38s \tline  28000000\n",
      "\t\tobjects in phrase\n",
      "\t\tobjects in chapter\n",
      "\t\tobjects in book\n",
      "\t\tobjects in clause\n",
      "\t\tobjects in clause\n",
      "     2m 43s \tline  29000000\n",
      "\t\tobjects in half_verse\n",
      "\t\tobjects in verse\n",
      "\t\tobjects in phrase_atom\n",
      "     2m 47s \tline  30000000\n",
      "\t\tobjects in phrase_atom\n",
      "     2m 50s \tline  31000000\n",
      "\t\tobjects in phrase_atom\n",
      "\t\tobjects in phrase_atom\n",
      "     2m 54s \tline  32000000\n",
      "\t\tobjects in phrase_atom\n",
      "     2m 58s \tline  33000000\n",
      "\t\tobjects in phrase_atom\n",
      "     2m 59s 33367199 lines parsed\n",
      "426581 objects of type word\n",
      "90562 objects of type clause_atom\n",
      "63570 objects of type sentence\n",
      "64339 objects of type sentence_atom\n",
      "113792 objects of type subphrase\n",
      "253174 objects of type phrase\n",
      "929 objects of type chapter\n",
      "39 objects of type book\n",
      "88000 objects of type clause\n",
      "45180 objects of type half_verse\n",
      "23213 objects of type verse\n",
      "267515 objects of type phrase_atom\n"
     ]
    }
   ],
   "source": [
    "parseMql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2m 59s Making TF data ...\n",
      "     2m 59s Monad - idd mapping ...\n",
      "     3m 00s maxSlot=426581\n",
      "     3m 00s Node mapping and otype ...\n",
      "     3m 00s oslots ...\n",
      "     3m 01s metadata ...\n",
      "     3m 01s features ...\n",
      "     3m 01s \tfeatures from words\n",
      "     3m 04s \t   100000 words\n",
      "     3m 08s \t   200000 words\n",
      "     3m 12s \t   300000 words\n",
      "     3m 16s \t   400000 words\n",
      "     3m 17s \t   426581 words\n",
      "     3m 17s \tfeatures from books\n",
      "     3m 17s \t       39 books\n",
      "     3m 17s \tfeatures from chapters\n",
      "     3m 17s \t      929 chapters\n",
      "     3m 17s \tfeatures from clauses\n",
      "     3m 18s \t    88000 clauses\n",
      "     3m 18s \tfeatures from clause_atoms\n",
      "     3m 20s \t    90562 clause_atoms\n",
      "     3m 20s \tfeatures from half_verses\n",
      "     3m 20s \t    45180 half_verses\n",
      "     3m 20s \tfeatures from phrases\n",
      "     3m 21s \t   100000 phrases\n",
      "     3m 22s \t   200000 phrases\n",
      "     3m 22s \t   253174 phrases\n",
      "     3m 22s \tfeatures from phrase_atoms\n",
      "     3m 23s \t   100000 phrase_atoms\n",
      "     3m 24s \t   200000 phrase_atoms\n",
      "     3m 25s \t   267515 phrase_atoms\n",
      "     3m 25s \tfeatures from sentences\n",
      "     3m 25s \t    63570 sentences\n",
      "     3m 25s \tfeatures from sentence_atoms\n",
      "     3m 25s \t    64339 sentence_atoms\n",
      "     3m 25s \tfeatures from subphrases\n",
      "     3m 26s \t   100000 subphrases\n",
      "     3m 26s \t   113792 subphrases\n",
      "     3m 26s \tfeatures from verses\n",
      "     3m 26s \t    23213 verses\n",
      "     3m 26s book names ...\n",
      "     3m 26s write data set to TF ...\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "\n",
      "  0.00s Grid feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 90 node and 4 edge and 1 config features to /Users/dirk/github/etcbc/bhsa/_temp/c/core:\n",
      "   |     0.05s T book                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@am              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@ar              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@bn              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@da              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@de              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@el              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@en              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@es              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@fa              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@fr              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@he              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@hi              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@id              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@ja              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@ko              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@la              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@nl              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@pa              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@pt              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@ru              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@sw              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@syc             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@tr              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@ur              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@yo              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T book@zh              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.08s T chapter              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.18s T code                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.89s T det                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     1.19s T dist                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     1.04s T dist_unit            to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.13s T domain               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.40s T function             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T g_cons               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T g_cons_utf8          to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.77s T g_lex                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T g_lex_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.71s T g_nme                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.68s T g_nme_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.68s T g_pfm                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.64s T g_pfm_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.63s T g_prs                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.63s T g_prs_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.63s T g_uvf                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.65s T g_uvf_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T g_vbe                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T g_vbe_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.64s T g_vbs                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.65s T g_vbs_utf8           to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.72s T g_voc_lex            to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.80s T g_voc_lex_utf8       to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T g_word               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.83s T g_word_utf8          to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.70s T gn                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.15s T is_root              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.15s T kind                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.10s T label                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.71s T language             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.72s T lex                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.80s T lex_utf8             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T ls                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.35s T mother_object_type   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.69s T nme                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.70s T nu                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     2.02s T number               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T otype                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.69s T pdp                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.73s T pfm                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.72s T prs                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T prs_gn               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.75s T prs_nu               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.72s T prs_ps               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.70s T ps                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.63s T qere                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.63s T qere_utf8            to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     1.20s T rela                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.70s T sp                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.76s T st                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.14s T tab                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T trailer              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T trailer_utf8         to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.16s T txt                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     1.18s T typ                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.77s T uvf                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.76s T vbe                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.76s T vbs                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.05s T verse                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.74s T vs                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.73s T vt                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     2.62s T distributional_parent to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     3.68s T functional_parent    to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.59s T mother               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     4.33s T oslots               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s M otext                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "    53s Exported 90 node features and 4 edge features and 1 config features to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n"
     ]
    }
   ],
   "source": [
    "tfFromData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s checkDiffs\n",
      "2 features to add:\n",
      "\tg_voc_lex g_voc_lex_utf8\n",
      "6 features to delete:\n",
      "\tgloss lex0 nametype root voc_lex voc_lex_utf8\n",
      "93 features in common\n",
      "book                      ... no changes\n",
      "book@am                   ... no changes\n",
      "book@ar                   ... no changes\n",
      "book@bn                   ... no changes\n",
      "book@da                   ... no changes\n",
      "book@de                   ... no changes\n",
      "book@el                   ... no changes\n",
      "book@en                   ... no changes\n",
      "book@es                   ... no changes\n",
      "book@fa                   ... no changes\n",
      "book@fr                   ... no changes\n",
      "book@he                   ... no changes\n",
      "book@hi                   ... no changes\n",
      "book@id                   ... no changes\n",
      "book@ja                   ... no changes\n",
      "book@ko                   ... no changes\n",
      "book@la                   ... no changes\n",
      "book@nl                   ... no changes\n",
      "book@pa                   ... no changes\n",
      "book@pt                   ... no changes\n",
      "book@ru                   ... no changes\n",
      "book@sw                   ... no changes\n",
      "book@syc                  ... no changes\n",
      "book@tr                   ... no changes\n",
      "book@ur                   ... no changes\n",
      "book@yo                   ... no changes\n",
      "book@zh                   ... no changes\n",
      "chapter                   ... no changes\n",
      "code                      ... no changes\n",
      "det                       ... no changes\n",
      "dist                      ... no changes\n",
      "dist_unit                 ... no changes\n",
      "distributional_parent     ... no changes\n",
      "domain                    ... no changes\n",
      "function                  ... no changes\n",
      "functional_parent         ... no changes\n",
      "g_cons                    ... no changes\n",
      "g_cons_utf8               ... no changes\n",
      "g_lex                     ... no changes\n",
      "g_lex_utf8                ... no changes\n",
      "g_nme                     ... no changes\n",
      "g_nme_utf8                ... no changes\n",
      "g_pfm                     ... no changes\n",
      "g_pfm_utf8                ... no changes\n",
      "g_prs                     ... no changes\n",
      "g_prs_utf8                ... no changes\n",
      "g_uvf                     ... no changes\n",
      "g_uvf_utf8                ... no changes\n",
      "g_vbe                     ... no changes\n",
      "g_vbe_utf8                ... no changes\n",
      "g_vbs                     ... no changes\n",
      "g_vbs_utf8                ... no changes\n",
      "g_word                    ... no changes\n",
      "g_word_utf8               ... no changes\n",
      "gn                        ... no changes\n",
      "is_root                   ... no changes\n",
      "kind                      ... no changes\n",
      "label                     ... no changes\n",
      "language                  ... First diff in line 2 after the metadata\n",
      "OLD -->hbo\n",
      "<--\n",
      "NEW -->Hebrew\n",
      "<--\n",
      "First diff in line 3 after the metadata\n",
      "OLD -->hbo\n",
      "<--\n",
      "NEW -->Hebrew\n",
      "<--\n",
      "\n",
      "lex                       ... First diff in line 426583 after the metadata\n",
      "OLD -->1436895\tB\n",
      "<--\n",
      "NEW --><empty><--\n",
      "\n",
      "lex_utf8                  ... First diff in line 2 after the metadata\n",
      "OLD -->הebrew\n",
      "<--\n",
      "NEW -->ב\n",
      "<--\n",
      "First diff in line 3 after the metadata\n",
      "OLD -->הebrew\n",
      "<--\n",
      "NEW -->ראשׁית֜\n",
      "<--\n",
      "\n",
      "ls                        ... First diff in line 426583 after the metadata\n",
      "OLD -->1436904\tvbcp\n",
      "<--\n",
      "NEW --><empty><--\n",
      "\n",
      "mother                    ... no changes\n",
      "mother_object_type        ... no changes\n",
      "nme                       ... no changes\n",
      "nu                        ... no changes\n",
      "number                    ... no changes\n",
      "oslots                    ... First diff in line 1010315 after the metadata\n",
      "OLD -->1,84,197,220,241,270,318,330,334,428,435,500,506,5<--\n",
      "NEW --><empty><--\n",
      "\n",
      "otext                     ... First diff in line 5 \n",
      "OLD -->@dateWritten=2017-09-27T20:36:00Z\n",
      "<--\n",
      "NEW -->@email=shebanq@ancient-data.org\n",
      "<--\n",
      "\n",
      "otype                     ... First diff in line 14 after the metadata\n",
      "OLD -->1436895-1446130\tlex\n",
      "<--\n",
      "NEW --><empty><--\n",
      "\n",
      "pdp                       ... no changes\n",
      "pfm                       ... no changes\n",
      "prs                       ... no changes\n",
      "prs_gn                    ... no changes\n",
      "prs_nu                    ... no changes\n",
      "prs_ps                    ... no changes\n",
      "ps                        ... no changes\n",
      "qere                      ... no changes\n",
      "qere_utf8                 ... no changes\n",
      "rela                      ... no changes\n",
      "sp                        ... First diff in line 426583 after the metadata\n",
      "OLD -->1436895\tprep\n",
      "<--\n",
      "NEW --><empty><--\n",
      "\n",
      "st                        ... no changes\n",
      "tab                       ... no changes\n",
      "trailer                   ... no changes\n",
      "trailer_utf8              ... no changes\n",
      "txt                       ... no changes\n",
      "typ                       ... no changes\n",
      "uvf                       ... no changes\n",
      "vbe                       ... no changes\n",
      "vbs                       ... no changes\n",
      "verse                     ... no changes\n",
      "vs                        ... no changes\n",
      "vt                        ... no changes\n"
     ]
    }
   ],
   "source": [
    "utils.checkDiffs(thisSave, thisDeliver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy data set to /Users/dirk/github/etcbc/bhsa/tf/c/core\n"
     ]
    }
   ],
   "source": [
    "utils.deliverDataset(thisSave, thisDeliver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s compileTfData\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "95 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     1.29s T otype                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |       12s T oslots               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.08s T book                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.05s T chapter              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.04s T verse                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.52s T g_cons               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.60s T g_cons_utf8          from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.51s T g_lex                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.75s T g_lex_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.56s T g_word               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.72s T g_word_utf8          from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.56s T lex                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.73s T lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.33s T trailer              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.35s T trailer_utf8         from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |      |     1.44s C __levels__           from otype, oslots\n",
      "   |      |       18s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.96s C __rank__             from otype, __order__\n",
      "   |      |       18s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |       10s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     4.40s C __boundary__         from otype, oslots, __rank__\n",
      "   |     0.00s M otext                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |      |     0.13s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.00s T book@am              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@ar              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@bn              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@da              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@de              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@el              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@en              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@es              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@fa              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@fr              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@he              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@hi              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@id              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@ja              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@ko              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@la              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@nl              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@pa              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@pt              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@ru              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@sw              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@syc             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@tr              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@ur              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@yo              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s T book@zh              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s Feature overview: 90 for nodes; 4 for edges; 1 configs; 7 computed\n",
      " 1m 22s All features loaded/computed - for details use loadLog()\n",
      "   |     0.00s Feature overview: 90 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "book book@am book@ar book@bn book@da book@de book@el book@en book@es book@fa book@fr book@he book@hi book@id book@ja book@ko book@la book@nl book@pa book@pt book@ru book@sw book@syc book@tr book@ur book@yo book@zh chapter code det dist dist_unit domain function g_cons g_cons_utf8 g_lex g_lex_utf8 g_nme g_nme_utf8 g_pfm g_pfm_utf8 g_prs g_prs_utf8 g_uvf g_uvf_utf8 g_vbe g_vbe_utf8 g_vbs g_vbs_utf8 g_voc_lex g_voc_lex_utf8 g_word g_word_utf8 gn is_root kind label language lex lex_utf8 ls mother_object_type nme nu number otype pdp pfm prs prs_gn prs_nu prs_ps ps qere qere_utf8 rela sp st tab trailer trailer_utf8 txt typ uvf vbe vbs verse vs vt distributional_parent functional_parent mother oslots\n",
      "  0.00s loading features ...\n",
      "   |     0.19s T code                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.82s T det                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.33s T dist                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     2.11s T dist_unit            from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     4.49s T distributional_parent from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.27s T domain               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.88s T function             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     6.43s T functional_parent    from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.98s T g_nme                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.14s T g_nme_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.75s T g_pfm                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.72s T g_pfm_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.81s T g_prs                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.75s T g_prs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.67s T g_uvf                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.66s T g_uvf_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.81s T g_vbe                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.73s T g_vbe_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.70s T g_vbs                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.69s T g_vbs_utf8           from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.57s T g_voc_lex            from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.70s T g_voc_lex_utf8       from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.41s T gn                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.30s T is_root              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.31s T kind                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.22s T label                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.53s T language             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.51s T ls                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.93s T mother               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.75s T mother_object_type   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.37s T nme                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.65s T nu                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     2.64s T number               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.73s T pdp                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.55s T pfm                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.78s T prs                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.55s T prs_gn               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.65s T prs_nu               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.62s T prs_ps               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.68s T ps                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.72s T qere                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.76s T qere_utf8            from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     2.86s T rela                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.50s T sp                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.51s T st                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.17s T tab                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.30s T txt                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     2.51s T typ                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.49s T uvf                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.43s T vbe                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.57s T vbs                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.60s T vs                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.72s T vt                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s Feature overview: 90 for nodes; 4 for edges; 1 configs; 7 computed\n",
      " 1m 16s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "compileTfData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
